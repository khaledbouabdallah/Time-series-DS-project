{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3112/3721340139.py:3: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.interpolate(method=\"linear\", direction = \"forward\", inplace= True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('exchange_rate_with_missing.csv')\n",
    "#fill missing values (does not work when the first values are missing)\n",
    "df.interpolate(method=\"linear\", direction = \"forward\", inplace= True)\n",
    "#change date type from object to date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "#set date as index\n",
    "df.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTM data should be normalized but in our example it is already in the range of 0 adn 1\n",
    "for x in df['6'] : \n",
    "    if x<0 and x>1 :\n",
    "        print(False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 134ms/step - loss: 4.1132e-06\n",
      "\u001b[1m 67/188\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 4.3158e-06 \n",
      "\u001b[1m136/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 4.3950e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 4.4443e-06\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 133ms/step - loss: 6.6039e-06\n",
      "\u001b[1m 63/188\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 3.1390e-06 \n",
      "\u001b[1m123/188\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 2.9557e-06\n",
      "\u001b[1m170/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 2.8995e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9111e-06  \n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 133ms/step - loss: 2.0004e-05\n",
      "\u001b[1m 53/188\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 2.1690e-05 \n",
      "\u001b[1m112/188\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 2.4113e-05\n",
      "\u001b[1m172/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 2.4499e-05\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 2.4926e-05\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 139ms/step - loss: 2.0206e-06\n",
      "\u001b[1m 57/188\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 2.4518e-06 \n",
      "\u001b[1m118/188\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 2.6722e-06\n",
      "\u001b[1m179/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 2.7696e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 2.7855e-06\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 132ms/step - loss: 4.5723e-05\n",
      "\u001b[1m 66/188\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 4.5615e-05 \n",
      "\u001b[1m133/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 4.4591e-05\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 4.4084e-05\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 142ms/step - loss: 2.4510e-06\n",
      "\u001b[1m 33/188\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6414e-06   \n",
      "\u001b[1m 90/188\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2670e-06\n",
      "\u001b[1m150/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3440e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 4.5764e-06\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 134ms/step - loss: 1.6271e-04\n",
      "\u001b[1m 61/188\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 1.6119e-04 \n",
      "\u001b[1m125/188\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 1.6238e-04\n",
      "\u001b[1m184/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 1.6292e-04\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 1.6311e-04\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 139ms/step - loss: 4.0979e-06\n",
      "\u001b[1m 45/188\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4044e-06   \n",
      "\u001b[1m 91/188\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2070e-06\n",
      "\u001b[1m138/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1676e-06\n",
      "\u001b[1m185/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1881e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1928e-06\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 128ms/step - loss: 2.3784e-05\n",
      "\u001b[1m 64/188\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 2.1116e-05 \n",
      "\u001b[1m131/188\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 2.1073e-05\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 2.1326e-05\n",
      "\n",
      "\u001b[1m  1/188\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 129ms/step - loss: 5.0543e-06\n",
      "\u001b[1m 60/188\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 3.5967e-06 \n",
      "\u001b[1m121/188\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 3.7927e-06\n",
      "\u001b[1m166/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 3.8285e-06\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 3.8886e-06\n",
      "\n",
      "100%|██████████| 10/10 [01:04<00:00,  6.44s/trial, best loss: 2.4086414214252727e-06]\n",
      "Best parameters: {'batch_size': 1, 'epochs': 0, 'learning_rate': 0.022965568195725542, 'units': 2}\n"
     ]
    }
   ],
   "source": [
    "#add feature '6+1' for t+1 values\n",
    "df['6+1'] = df['6'].shift(-1) \n",
    "\n",
    "#separate train and test datasets \n",
    "index = int(len(df) * 0.8)\n",
    "train_data_x = df['6'].iloc[:index].values\n",
    "train_data_y = df['6+1'].iloc[:index].values\n",
    "test_data_x = df['6'].iloc[index:].values\n",
    "test_data_y = df['6+1'].iloc[index:].values\n",
    "\n",
    "#print(test_data_x.shape)\n",
    "\n",
    "#The LSTM network expects the input data (X) to be provided with a specific array structure in the form of [samples, time steps, features]\n",
    "train_data_x = train_data_x.reshape(train_data_x.shape[0], 1, 1)\n",
    "test_data_x = test_data_x.reshape(test_data_x.shape[0], 1, 1)\n",
    "\n",
    "def lstm_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['units'], input_shape=(1, 1)))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])  # Correctly specifying the learning rate here\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    model.fit(train_data_x, train_data_y, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "    # Evaluate the model on validation data and return the loss\n",
    "    val_loss = model.evaluate(train_data_x, train_data_y)\n",
    "    return val_loss\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'units': hp.choice('units', [50, 100, 150]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.0001, 0.1),\n",
    "    'epochs': hp.choice('epochs', [5, 10, 15]),\n",
    "    'batch_size': hp.choice('batch_size', [8, 16, 32])\n",
    "}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(params):\n",
    "    loss = lstm_model(params)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5990/5990 - 6s - 1ms/step - loss: 0.0016\n",
      "Epoch 2/5\n",
      "5990/5990 - 5s - 897us/step - loss: 2.2747e-05\n",
      "Epoch 3/5\n",
      "5990/5990 - 10s - 2ms/step - loss: 2.2308e-05\n",
      "Epoch 4/5\n",
      "5990/5990 - 10s - 2ms/step - loss: 2.2923e-05\n",
      "Epoch 5/5\n",
      "5990/5990 - 10s - 2ms/step - loss: 2.2922e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f09da0e6470>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#create and fit the LSTM network\n",
    "model = Sequential() \n",
    "model.add(LSTM(1, input_shape=(1, 1))) #4 LSTM blocks, 1 time step and 1 feature in each step\n",
    "model.add(Dense(1)) # 1 fully connected (dense) layer with one neurone (or block)\n",
    "optimizer = Adam(learning_rate= 0.022965568195725542)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer) #configures the model for training with the mean squared error loss function and the Adam optimizer\n",
    "model.fit(train_data_x, train_data_y, epochs=5, batch_size=1, verbose=2) #trains the compiled model on the training data for 10 epochs using stochastic gradient descent with a batch size of 1, and it prints progress output for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "(1498,)\n",
      "(1498,)\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "test_predictions = model.predict(train_data_x)\n",
    "test_predictions = test_predictions.flatten()\n",
    "test_predictions = test_predictions[:len(test_data_x)]\n",
    "test_data_xx = test_data_x.flatten()\n",
    "print(test_predictions.shape)\n",
    "print(test_data_xx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14705691304340035\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(test_data_xx, test_predictions)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 65/234\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 784us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "#prédire les 100 prochaines valeurs\n",
    "data = df['6'].values\n",
    "data = data.reshape(data.shape[0], 1, 1)\n",
    "predictions = model.predict(data)\n",
    "predictions = predictions.flatten()\n",
    "predictions = predictions[:100]\n",
    "print(predictions.shape)\n",
    "i = 0\n",
    "with open(\"lstm_results.csv\", 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['ID', 'Prediction'])\n",
    "    for x in predictions:\n",
    "        csv_writer.writerow([i, x])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
